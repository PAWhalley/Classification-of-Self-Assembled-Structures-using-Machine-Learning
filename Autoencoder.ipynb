{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jan 24 16:02:40 2021\n",
    "\n",
    "@author: s2110992\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "#mnist_data = datasets.MNIST('data',train=True, download = True, transform = transforms.ToTensor())\n",
    "#mnist_data = list(mnist_data)[:4096] #data of size powers of 2 works better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_size = 50\n",
    "data_size = 5\n",
    "data = []\n",
    "#for i in range(training_data_size):\n",
    "#    data.append(torch.rand(data_size))\n",
    "#data_tensor = torch.rand((training_data_size,data_size))\n",
    "training_set = np.random.uniform(size =(training_data_size,data_size))\n",
    "data_tensor = torch.FloatTensor(training_set)\n",
    "#print(data_tensor[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note we can make BATCH_SIZE smaller than training data set then it \n",
    "#does iterative gradient steps.\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50 \n",
    "BATCH_SIZE = training_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=5, out_features=49, bias=True)\n",
      "  (enc2): Linear(in_features=49, out_features=48, bias=True)\n",
      "  (dec1): Linear(in_features=48, out_features=49, bias=True)\n",
      "  (dec2): Linear(in_features=49, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0895, 0.1500, 0.0000, 0.1218], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the architecture of the Neural Network\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=data_size, out_features=49)\n",
    "        self.enc2 = nn.Linear(in_features = 49,out_features = 48)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=48, out_features=49)\n",
    "        self.dec2 = nn.Linear(in_features = 49, out_features =data_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        \n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        return x\n",
    "net = Autoencoder()\n",
    "print(net)\n",
    "net(torch.rand(data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mean Square Error as the Loss function.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.795409072190523\n",
      "9.462770164944232\n",
      "9.199916643556207\n",
      "8.750967706553638\n",
      "4.718390598893166\n",
      "1.7729347876738757\n",
      "1.3416945544304326\n",
      "0.9958307124907151\n",
      "0.7095805676653981\n",
      "0.5274274164112285\n",
      "0.38652235618792474\n",
      "0.246155898028519\n",
      "0.10909972910303622\n",
      "0.046685890112712514\n",
      "0.03365746103190759\n",
      "0.02679124476344441\n",
      "0.023138798449053866\n",
      "0.020141917385444685\n",
      "0.01832888928402099\n",
      "0.016891962763111223\n",
      "0.015862428046602872\n",
      "0.015121676267881412\n",
      "0.01440534998619114\n",
      "0.013946200740974746\n",
      "0.013622482141727232\n",
      "0.013495684586814605\n",
      "0.013061570291029057\n",
      "0.01278876042124466\n",
      "0.012613183214853052\n",
      "0.012121091353037627\n",
      "0.011845630566313048\n",
      "0.011610640616709134\n",
      "0.011057609697672888\n",
      "0.010887588021432748\n",
      "0.01156166710825346\n",
      "0.009804220839214395\n",
      "0.009570855319907423\n",
      "0.00951796284516604\n",
      "0.009207857605360914\n",
      "0.008643802158985636\n",
      "0.008009533454242046\n",
      "0.007650775949514355\n",
      "0.007426720261719311\n",
      "0.007013581387582235\n",
      "0.007005943340118392\n",
      "0.007028634141533985\n",
      "0.007200047955848277\n",
      "0.007040727190542384\n",
      "0.007067301072311238\n",
      "0.0068670913892674434\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Training Step!.\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(training_data_size):\n",
    "        # get the inputs;\n",
    "        inputs = data_tensor[i]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(running_loss)\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "         #   print('[%d, %5d] loss: %.3f' %\n",
    "          #        (epoch + 1, i + 1, running_loss / 2000))\n",
    "           # running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Hidden Layer: Lower Dimensional Representation of our data and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
